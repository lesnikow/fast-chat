# Results

# dpo vs dcpo using reference implementation for sft and dpo


## Single
python show_result.py --input-file data/mt_bench/model_judgment/gpt-4-turbo_single.jsonl

Mode: single
Input file: data/mt_bench/model_judgment/gpt-4-turbo_single.jsonl

########## First turn ##########
                                              score
model                                   turn       
b_dcpo_policy_answers_max_new_token_128 1     2.200
a_dpo_model_answers_max_token_128       1     2.125

########## Second turn ##########
                                               score
model                                   turn        
b_dcpo_policy_answers_max_new_token_128 2     1.7125
a_dpo_model_answers_max_token_128       2     1.4875

########## Average ##########
                                           score
model                                           
b_dcpo_policy_answers_max_new_token_128  1.95625
a_dpo_model_answers_max_token_128        1.80625


## Pairwise
python show_result.py --mode pairwise-all --input-file data/mt_bench/model_judgment/gpt-4-turbo_pair.jsonl 

Mode: pairwise-all
Input file: data/mt_bench/model_judgment/gpt-4-turbo_pair.jsonl
                                         win  loss  tie  win_rate  loss_rate  win_rate_adjusted
model                                                                                          
b_dcpo_policy_answers_max_new_token_128   29    18  123  0.170588   0.105882           0.532353
a_dpo_model_answers_max_token_128         18    29  123  0.105882   0.170588           0.467647

