# Results
# dpo vs dcpo using reference implementation for sft and dpo

## Haiku 11 voters mp vs rv

Mode: single
Input file: data/mt_bench/model_judgment/gpt-4-turbo_single.jsonl

########## First turn ##########
                                                          score
model                                              turn        
mp_11_haiku_voters_dataset_dpo_loss_pythia28_mo... 1     2.4000
rv_11_haiku_voters_dataset_dpo_loss_pythia28_mo... 1     2.2125

########## Second turn ##########
                                                          score
model                                              turn        
mp_11_haiku_voters_dataset_dpo_loss_pythia28_mo... 2     1.6625
rv_11_haiku_voters_dataset_dpo_loss_pythia28_mo... 2     1.5750

########## Average ##########
                                                      score
model                                                      
mp_11_haiku_voters_dataset_dpo_loss_pythia28_mo...  2.03125
rv_11_haiku_voters_dataset_dpo_loss_pythia28_mo...  1.89375


Mode: pairwise-all
Input file: data/mt_bench/model_judgment/gpt-4-turbo_pair.jsonl
                                                    win  loss  tie  win_rate  loss_rate  win_rate_adjusted
model                                                                                                     
mp_11_haiku_voters_dataset_dpo_loss_pythia28_mo...   28    16  115  0.176101   0.100629           0.537736
rv_11_haiku_voters_dataset_dpo_loss_pythia28_mo...   16    28  115  0.100629   0.176101           0.462264




## 33 voter experiments
### Mode single
Mode: single
Input file: data/mt_bench/model_judgment/gpt-4-turbo_single.jsonl

########## First turn ##########
                                                            score
model                                              turn          
av_answers                                         1     2.500000
hb_answers                                         1     2.426829
rmp_answers                                        1     2.292683
hb_answers_512_max_new_tokens                      1     2.287500
rv_answers                                         1     2.280488
mp_answers                                         1     2.243902
b_dcpo_policy_answers_max_new_token_128            1     2.200000
mp_33_voters_dataset_dpo_loss_pythia28_32_batch... 1     2.200000
a_dpo_model_answers_max_token_128                  1     2.125000
av_answers_512_max_new_tokens                      1     2.125000
mp_answers_512_max_new_tokens                      1     2.062500
rmp_answers_512_max_new_tokens                     1     2.025000
rv_answers_512_max_new_tokens                      1     2.025000
rv_33_voters_dataset_dpo_loss_pythia28_32_batch... 1     1.912500

########## Second turn ##########
                                                            score
model                                              turn          
rmp_answers_512_max_new_tokens                     2     1.887500
rmp_answers                                        2     1.865854
av_answers                                         2     1.804878
hb_answers                                         2     1.780488
b_dcpo_policy_answers_max_new_token_128            2     1.712500
mp_answers                                         2     1.682927
rv_answers                                         2     1.670732
rv_answers_512_max_new_tokens                      2     1.662500
mp_answers_512_max_new_tokens                      2     1.650000
hb_answers_512_max_new_tokens                      2     1.575000
mp_33_voters_dataset_dpo_loss_pythia28_32_batch... 2     1.550000
a_dpo_model_answers_max_token_128                  2     1.487500
av_answers_512_max_new_tokens                      2     1.450000
rv_33_voters_dataset_dpo_loss_pythia28_32_batch... 2     1.437500

########## Average ##########
                                                       score
model                                                       
av_answers                                          2.152439
hb_answers                                          2.103659
rmp_answers                                         2.079268
rv_answers                                          1.975610
mp_answers                                          1.963415
b_dcpo_policy_answers_max_new_token_128             1.956250
rmp_answers_512_max_new_tokens                      1.956250
hb_answers_512_max_new_tokens                       1.931250
mp_33_voters_dataset_dpo_loss_pythia28_32_batch...  1.875000
mp_answers_512_max_new_tokens                       1.856250
rv_answers_512_max_new_tokens                       1.843750
a_dpo_model_answers_max_token_128                   1.806250
av_answers_512_max_new_tokens                       1.787500
rv_33_voters_dataset_dpo_loss_pythia28_32_batch...  1.675000



### Mode pairwise-all

python3 show_result.py   --mode "pairwise-all"   --judge-model "gpt-4-turbo"   --model-list "${model_answers[@]}"
Mode: pairwise-all
Input file: data/mt_bench/model_judgment/gpt-4-turbo_pair.jsonl
                                                    win  loss  tie  win_rate  loss_rate  win_rate_adjusted
model                                                                                                     
mp_33_voters_dataset_dpo_loss_pythia28_32_batch...   26    15  119   0.16250    0.09375           0.534375
rv_33_voters_dataset_dpo_loss_pythia28_32_batch...   15    26  119   0.09375    0.16250           0.465625



## Five-arm experiment


python3 show_result.py   --mode single   --judge-model "gpt-4-turbo"
Mode: single
Input file: data/mt_bench/model_judgment/gpt-4-turbo_single.jsonl

########## First turn ##########
                                                 score
model                                   turn          
av_answers                              1     2.500000
hb_answers                              1     2.426829
rmp_answers                             1     2.292683
hb_answers_512_max_new_tokens           1     2.287500
rv_answers                              1     2.280488
mp_answers                              1     2.243902
b_dcpo_policy_answers_max_new_token_128 1     2.200000
a_dpo_model_answers_max_token_128       1     2.125000
av_answers_512_max_new_tokens           1     2.125000
mp_answers_512_max_new_tokens           1     2.062500
rmp_answers_512_max_new_tokens          1     2.025000
rv_answers_512_max_new_tokens           1     2.025000

########## Second turn ##########
                                                 score
model                                   turn          
rmp_answers_512_max_new_tokens          2     1.887500
rmp_answers                             2     1.865854
av_answers                              2     1.804878
hb_answers                              2     1.780488
b_dcpo_policy_answers_max_new_token_128 2     1.712500
mp_answers                              2     1.682927
rv_answers                              2     1.670732
rv_answers_512_max_new_tokens           2     1.662500
mp_answers_512_max_new_tokens           2     1.650000
hb_answers_512_max_new_tokens           2     1.575000
a_dpo_model_answers_max_token_128       2     1.487500
av_answers_512_max_new_tokens           2     1.450000

########## Average ##########
                                            score
model                                            
av_answers                               2.152439
hb_answers                               2.103659
rmp_answers                              2.079268
rv_answers                               1.975610
mp_answers                               1.963415
rmp_answers_512_max_new_tokens           1.956250
b_dcpo_policy_answers_max_new_token_128  1.956250
hb_answers_512_max_new_tokens            1.931250
mp_answers_512_max_new_tokens            1.856250
rv_answers_512_max_new_tokens            1.843750
a_dpo_model_answers_max_token_128        1.806250
av_answers_512_max_new_tokens            1.787500


python3 show_result.py   --mode single   --judge-model "gpt-4-turbo"
Mode: single
Input file: data/mt_bench/model_judgment/gpt-4-turbo_single.jsonl

########## First turn ##########
                                                 score
model                                   turn
av_answers                              1     2.500000
hb_answers                              1     2.426829
rmp_answers                             1     2.292683
rv_answers                              1     2.280488
mp_answers                              1     2.243902
b_dcpo_policy_answers_max_new_token_128 1     2.200000
a_dpo_model_answers_max_token_128       1     2.125000

########## Second turn ##########
                                                 score
model                                   turn
rmp_answers                             2     1.865854
av_answers                              2     1.804878
hb_answers                              2     1.780488
b_dcpo_policy_answers_max_new_token_128 2     1.712500
mp_answers                              2     1.682927
rv_answers                              2     1.670732
a_dpo_model_answers_max_token_128       2     1.487500

########## Average ##########
                                            score
model
av_answers                               2.152439
hb_answers                               2.103659
rmp_answers                              2.079268
rv_answers                               1.975610
mp_answers                               1.963415
b_dcpo_policy_answers_max_new_token_128  1.956250
a_dpo_model_answers_max_token_128        1.806250







## Two-arm experiment

### Single
python show_result.py --input-file data/mt_bench/model_judgment/gpt-4-turbo_single.jsonl

Mode: single
Input file: data/mt_bench/model_judgment/gpt-4-turbo_single.jsonl

########## First turn ##########
                                              score
model                                   turn       
b_dcpo_policy_answers_max_new_token_128 1     2.200
a_dpo_model_answers_max_token_128       1     2.125

########## Second turn ##########
                                               score
model                                   turn        
b_dcpo_policy_answers_max_new_token_128 2     1.7125
a_dpo_model_answers_max_token_128       2     1.4875

########## Average ##########
                                           score
model                                           
b_dcpo_policy_answers_max_new_token_128  1.95625
a_dpo_model_answers_max_token_128        1.80625


### Pairwise
python show_result.py --mode pairwise-all --input-file data/mt_bench/model_judgment/gpt-4-turbo_pair.jsonl 

Mode: pairwise-all
Input file: data/mt_bench/model_judgment/gpt-4-turbo_pair.jsonl
                                         win  loss  tie  win_rate  loss_rate  win_rate_adjusted
model                                                                                          
b_dcpo_policy_answers_max_new_token_128   29    18  123  0.170588   0.105882           0.532353
a_dpo_model_answers_max_token_128         18    29  123  0.105882   0.170588           0.467647

Mode: single
Input file: data/mt_bench/model_judgment/gpt-4_single.jsonl

########## First turn ##########
                                           score
model                             turn          
gpt-4                             1     8.956250
claude-v1                         1     8.150000
gpt-3.5-turbo                     1     8.075000
claude-instant-v1                 1     7.800000
vicuna-33b-v1.3                   1     7.456250
wizardlm-30b                      1     7.131250
wizardlm-13b                      1     7.118750
oasst-sft-7-llama-30b             1     7.106250
Llama-2-13b-chat                  1     7.062500
tulu-30b                          1     7.018750
Llama-2-70b-chat                  1     6.987500
guanaco-33b                       1     6.881250
vicuna-13b-v1.3                   1     6.812500
guanaco-65b                       1     6.781250
palm-2-chat-bison-001             1     6.712500
vicuna-7b-v1.3                    1     6.693750
mpt-30b-chat                      1     6.675000
nous-hermes-13b                   1     6.431250
Llama-2-7b-chat                   1     6.412500
baize-v2-13b                      1     6.318750
gpt4all-13b-snoozy                1     6.075000
koala-13b                         1     6.075000
mpt-7b-chat                       1     5.850000
falcon-40b-instruct               1     5.812500
mpt-30b-instruct                  1     5.675000
h2ogpt-oasst-open-llama-13b       1     5.512500
chatglm-6b                        1     5.000000
alpaca-13b                        1     4.975000
oasst-sft-4-pythia-12b            1     4.975000
rwkv-4-raven-14b                  1     4.743750
dolly-v2-12b                      1     3.800000
fastchat-t5-3b                    1     3.393750
llama-13b                         1     3.262500
stablelm-tuned-alpha-7b           1     2.968750
a_dpo_model_answers_max_token_128 1     1.916667

########## Second turn ##########
                                           score
model                             turn          
gpt-4                             2     9.025000
claude-instant-v1                 2     8.012658
gpt-3.5-turbo                     2     7.812500
claude-v1                         2     7.650000
wizardlm-30b                      2     6.887500
vicuna-33b-v1.3                   2     6.787500
Llama-2-70b-chat                  2     6.725000
Llama-2-13b-chat                  2     6.237500
guanaco-33b                       2     6.175000
Llama-2-7b-chat                   2     6.125000
mpt-30b-chat                      2     6.112500
palm-2-chat-bison-001             2     6.087500
guanaco-65b                       2     6.037500
vicuna-13b-v1.3                   2     5.962500
tulu-30b                          2     5.850000
oasst-sft-7-llama-30b             2     5.712500
wizardlm-13b                      2     5.587500
vicuna-7b-v1.3                    2     5.300000
baize-v2-13b                      2     5.181250
mpt-7b-chat                       2     5.063291
gpt4all-13b-snoozy                2     4.822785
mpt-30b-instruct                  2     4.762500
nous-hermes-13b                   2     4.664557
koala-13b                         2     4.625000
falcon-40b-instruct               2     4.525000
alpaca-13b                        2     4.087500
chatglm-6b                        2     4.000000
h2ogpt-oasst-open-llama-13b       2     3.737500
oasst-sft-4-pythia-12b            2     3.662500
rwkv-4-raven-14b                  2     3.225000
dolly-v2-12b                      2     2.750000
fastchat-t5-3b                    2     2.687500
stablelm-tuned-alpha-7b           2     2.537500
llama-13b                         2     1.950000
a_dpo_model_answers_max_token_128 2     1.083333

########## Average ##########
                                      score
model                                      
gpt-4                              8.990625
gpt-3.5-turbo                      7.943750
claude-instant-v1                  7.905660
claude-v1                          7.900000
vicuna-33b-v1.3                    7.121875
wizardlm-30b                       7.009375
Llama-2-70b-chat                   6.856250
Llama-2-13b-chat                   6.650000
guanaco-33b                        6.528125
tulu-30b                           6.434375
guanaco-65b                        6.409375
oasst-sft-7-llama-30b              6.409375
palm-2-chat-bison-001              6.400000
mpt-30b-chat                       6.393750
vicuna-13b-v1.3                    6.387500
wizardlm-13b                       6.353125
Llama-2-7b-chat                    6.268750
vicuna-7b-v1.3                     5.996875
baize-v2-13b                       5.750000
nous-hermes-13b                    5.553459
mpt-7b-chat                        5.459119
gpt4all-13b-snoozy                 5.452830
koala-13b                          5.350000
mpt-30b-instruct                   5.218750
falcon-40b-instruct                5.168750
h2ogpt-oasst-open-llama-13b        4.625000
alpaca-13b                         4.531250
chatglm-6b                         4.500000
oasst-sft-4-pythia-12b             4.318750
rwkv-4-raven-14b                   3.984375
dolly-v2-12b                       3.275000
fastchat-t5-3b                     3.040625
stablelm-tuned-alpha-7b            2.753125
llama-13b                          2.606250
a_dpo_model_answers_max_token_128  1.500000
