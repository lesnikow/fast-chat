# Results
# dpo vs dcpo using reference implementation for sft and dpo


## Five-arm experiment

python3 show_result.py   --mode single   --judge-model "gpt-4-turbo"

Mode: single
Input file: data/mt_bench/model_judgment/gpt-4-turbo_single.jsonl

########## First turn ##########
                                                 score
model                                   turn
av_answers                              1     2.500000
hb_answers                              1     2.426829
rmp_answers                             1     2.292683
rv_answers                              1     2.280488
mp_answers                              1     2.243902
b_dcpo_policy_answers_max_new_token_128 1     2.200000
a_dpo_model_answers_max_token_128       1     2.125000

########## Second turn ##########
                                                 score
model                                   turn
rmp_answers                             2     1.865854
av_answers                              2     1.804878
hb_answers                              2     1.780488
b_dcpo_policy_answers_max_new_token_128 2     1.712500
mp_answers                              2     1.682927
rv_answers                              2     1.670732
a_dpo_model_answers_max_token_128       2     1.487500

########## Average ##########
                                            score
model
av_answers                               2.152439
hb_answers                               2.103659
rmp_answers                              2.079268
rv_answers                               1.975610
mp_answers                               1.963415
b_dcpo_policy_answers_max_new_token_128  1.956250
a_dpo_model_answers_max_token_128        1.806250







## Two-arm experiment

### Single
python show_result.py --input-file data/mt_bench/model_judgment/gpt-4-turbo_single.jsonl

Mode: single
Input file: data/mt_bench/model_judgment/gpt-4-turbo_single.jsonl

########## First turn ##########
                                              score
model                                   turn       
b_dcpo_policy_answers_max_new_token_128 1     2.200
a_dpo_model_answers_max_token_128       1     2.125

########## Second turn ##########
                                               score
model                                   turn        
b_dcpo_policy_answers_max_new_token_128 2     1.7125
a_dpo_model_answers_max_token_128       2     1.4875

########## Average ##########
                                           score
model                                           
b_dcpo_policy_answers_max_new_token_128  1.95625
a_dpo_model_answers_max_token_128        1.80625


### Pairwise
python show_result.py --mode pairwise-all --input-file data/mt_bench/model_judgment/gpt-4-turbo_pair.jsonl 

Mode: pairwise-all
Input file: data/mt_bench/model_judgment/gpt-4-turbo_pair.jsonl
                                         win  loss  tie  win_rate  loss_rate  win_rate_adjusted
model                                                                                          
b_dcpo_policy_answers_max_new_token_128   29    18  123  0.170588   0.105882           0.532353
a_dpo_model_answers_max_token_128         18    29  123  0.105882   0.170588           0.467647

